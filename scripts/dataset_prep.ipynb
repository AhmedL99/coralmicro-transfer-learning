{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A \n",
    "import cv2\n",
    "import numpy as np \n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset mergeing\n",
    "Dataset is gathered form 2 differenct sources: \n",
    "- [../](acquisition dataset): This project consists of taking images of myself using the coralmicro. \n",
    "- [](): kaggle dataset with Yolo format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 324\n",
    "IMG_HEIGHT = IMG_WIDTH\n",
    "\n",
    "# Path to the images folder\n",
    "images_path = \"/home/ahmed/Desktop/coralmicro-transfer-learning/dataset/merged/images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "annotations = []\n",
    "\n",
    "categories = [\n",
    "    {\n",
    "      \"id\": 0,\n",
    "      \"name\": \"Ahmed\"\n",
    "    }, \n",
    "    {\n",
    "      \"id\": 1,\n",
    "      \"name\": \"not_Ahmed\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ahmed dataset \n",
    "\n",
    "img_idx = 0 \n",
    "anns_idx = 0\n",
    "\n",
    "# Load images and their anns from the 1st class\n",
    "ahmed_dataset_json_file = \"../dataset/Ahmed/result.json\"\n",
    "coco_ahmed = COCO(annotation_file=ahmed_dataset_json_file)\n",
    "ahmed_imgs_ids = coco_ahmed.getImgIds()\n",
    "ahmed_imgs = coco_ahmed.loadImgs(ids=ahmed_imgs_ids)\n",
    "\n",
    "\n",
    "for img in ahmed_imgs: \n",
    "\n",
    "    # Change image file name\n",
    "    #in_img_path = os.path.join(images_path, img[\"file_name\"][16:])\n",
    "    #out_img_path = os.path.join(images_path, str(img_idx))\n",
    "    #os.rename(in_img_path, out_img_path)\n",
    "    \n",
    "    img[\"file_name\"] = f\"{str(img_idx)}.jpg\"\n",
    "    images.append(img)\n",
    "    annotations.append(coco_ahmed.loadAnns(ids=img[\"id\"])[0])\n",
    "    #print(len(coco_ahmed.getAnnIds(imgIds=img[\"id\"])))\n",
    "    #print(coco_ahmed.loadAnns(ids=img[\"id\"]))\n",
    "\n",
    "    # Get the number of anns for all the ahmed dataset \n",
    "    anns_idx += len(coco_ahmed.getAnnIds(imgIds=img[\"id\"]))\n",
    "\n",
    "    # Increment the image index\n",
    "    img_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not Ahmed dataset \n",
    "\n",
    "# Load images and their anns from the 2nd class\n",
    "not_ahmed_dataset_json_file = \"../dataset/kaggle_face_detection/out_coco/annotations.json\"\n",
    "coco_not_ahmed = COCO(annotation_file=not_ahmed_dataset_json_file)\n",
    "not_ahmed_imgs_ids = coco_not_ahmed.getImgIds()\n",
    "not_ahmed_imgs = coco_not_ahmed.loadImgs(ids=not_ahmed_imgs_ids)\n",
    "\n",
    "for img in not_ahmed_imgs: \n",
    "    \n",
    "    # Change image filename\n",
    "    #in_img_path = os.path.join(images_path, img[\"file_name\"])\n",
    "    #out_img_path = os.path.join(images_path, str(img_idx))\n",
    "    #os.rename(in_img_path, out_img_path)\n",
    "\n",
    "    img[\"file_name\"] = f\"{str(img_idx)}.jpg\"\n",
    "    # Get the anns list\n",
    "    img_anns_ids = coco_not_ahmed.getAnnIds(imgIds=img[\"id\"])\n",
    "    for ann_id in  img_anns_ids: \n",
    "        ann = coco_not_ahmed.loadAnns(ids=ann_id)\n",
    "        ann[0][\"image_id\"]= int(img_idx)\n",
    "        ann[0][\"id\"] = anns_idx\n",
    "        annotations.append(ann[0])\n",
    "        anns_idx += 1\n",
    "\n",
    "    img[\"id\"] = img_idx\n",
    "    img_idx += 1\n",
    "    images.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the output json file \n",
    "output_json_file = \"../dataset/merged/annotations.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "# Write data to the output json file \n",
    "with open(output_json_file, 'w') as file:\n",
    "    json.dump(output_data, file, indent=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation & resizing\n",
    "After merging the data, we need know to resize the images with the same Coralmicro integrated camera sensor dims: 324x324p. \n",
    "We used albumentation library for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure augmentations \n",
    "pre_aug_json = output_json_file\n",
    "pre_aug = COCO(annotation_file=pre_aug_json)\n",
    "\n",
    "class_labels = ['Ahmed', 'not_Ahmed']\n",
    "aug_anns = []\n",
    "\n",
    "## Resize the images: 324 x 324\n",
    "Resize = A.Compose([\n",
    "    A.Resize(width=IMG_WIDTH, height=IMG_HEIGHT),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='coco',label_fields=['bbox_classes']))\n",
    "\n",
    "aug_path = \"/home/ahmed/Desktop/coralmicro-transfer-learning/dataset/aug\"\n",
    "aug_images = os.path.join(aug_path, \"images\")\n",
    "aug_json = os.path.join(aug_path, \"aug.json\")\n",
    "\n",
    "# Loop through all the images \n",
    "for img in images: \n",
    "    \n",
    "    arr_img = cv2.imread(os.path.join(images_path, img[\"file_name\"]))\n",
    "    #print(os.path.join(images_path, img[\"file_name\"]))\n",
    "    # Get anns for this image \n",
    "    img_bbox = []\n",
    "    bbox_classes = []\n",
    "    img_anns_ids = pre_aug.getAnnIds(imgIds=img[\"id\"])\n",
    "    nbr_anns = len(img_anns_ids)\n",
    "\n",
    "    # Collect bboxes for each ima\n",
    "    for ann_id in img_anns_ids: \n",
    "        ann = pre_aug.loadAnns(ids=ann_id)\n",
    "        #conc_bbox = ann[0][\"bbox\"] + [class_labels[ann[0][\"category_id\"]]]\n",
    "        img_bbox.append(ann[0][\"bbox\"])\n",
    "        bbox_classes.append(class_labels[ann[0][\"category_id\"]])\n",
    "\n",
    "    # Perform image resize \n",
    "    transformed = Resize(image=arr_img, bboxes=img_bbox, bbox_classes=bbox_classes)\n",
    "    transformed_image = transformed['image']\n",
    "    transformed_bboxes = transformed['bboxes']\n",
    "    transformed_class_labels = transformed['bbox_classes']\n",
    "\n",
    "\n",
    "    # Export new image\n",
    "    aug_filename = os.path.join(aug_images, img[\"file_name\"])\n",
    "    cv2.imwrite(aug_filename, transformed_image)\n",
    "    # Update Json file \n",
    "    img[\"file_name\"] = aug_filename \n",
    "    img[\"width\"] = IMG_WIDTH\n",
    "    img[\"height\"] = IMG_HEIGHT\n",
    "           \n",
    "    for ann_id, transformed_bbox, transformed_class_label in zip(img_anns_ids, transformed_bboxes, transformed_class_labels):\n",
    "        # Load the annotation using its ID\n",
    "        ann = pre_aug.loadAnns(ids=ann_id)\n",
    "    \n",
    "        # Update the bounding box\n",
    "        ann[0][\"bbox\"] = transformed_bbox\n",
    "\n",
    "        # Update the category_id based on the transformed class labels\n",
    "        ann[0][\"category_id\"] = 0 if transformed_class_label == \"Ahmed\" else 1\n",
    "\n",
    "              \n",
    "        print(ann)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the resized images in one folder \n",
    "# Export the resized Json file \n",
    "\n",
    "output_aug = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "# Write data to the output json file \n",
    "with open(aug_json, 'w') as file:\n",
    "  json.dump(output_data, file, indent=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the datasest\n",
    "We will finish with splitting the dataset into train and val and prepare it for the tfrecords conversion. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
